{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<h1>Curso Procesamiento de Lenguaje Natural</h1>\n",
    "\n",
    "<h3>NER con LSTMs</h3>\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "<p>\n",
    "<img src=\"https://identidadbuho.unison.mx/wp-content/uploads/2019/06/letragrama-cmyk-72.jpg\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/ner-lstm.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
    "\n",
    "Tomado parcialmente y adaptado de [el repositorio de github](https://github.com/Tekraj15/Named-Entity-Recognition-Using-LSTM-Keras/tree/master) \n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('GPU detected:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando datos para NER\n",
    "\n",
    "Para el NER, vanos a descargar un conjunto de datos de entrenamiento y prueba en español de la [Universidad de Antwerp](https://www.uantwerpen.be/en/), los cuales los puedes [consultar aquí](https://www.clips.uantwerpen.be/conll2002/ner/).\n",
    "\n",
    "Y pues lo más fácil es descargarlos usando linea de comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm esp.*\n",
    "!curl -O https://www.clips.uantwerpen.be/conll2002/ner/data/esp.train.gz\n",
    "!gunzip -f esp.train.gz \n",
    "!curl -O https://www.clips.uantwerpen.be/conll2002/ner/data/esp.testa.gz\n",
    "!gunzip -f esp.testa.gz \n",
    "!curl -O https://www.clips.uantwerpen.be/conll2002/ner/data/esp.testb.gz\n",
    "!gunzip -f esp.testb.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedes abrir un archivo para ver que son palabras, así como su etiqueta usando el método de marcado comocido como [*BIO*](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)).\n",
    "\n",
    "Vamos a leer un archivo y convertirlo a un `dataframe` de pandas para un manejo más fácil de la información. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carga_dataframe(nombre):\n",
    "\n",
    "  palabra = []\n",
    "  tag = []\n",
    "  sentencia = []\n",
    "\n",
    "  with open(nombre, encoding='latin1') as fp:\n",
    "    s = 1\n",
    "    for line in fp.readlines():\n",
    "      if len(line.strip()) == 0:\n",
    "        s += 1\n",
    "      else:\n",
    "        palabra.append(line.split()[0].strip())\n",
    "        tag.append(line.split()[1].strip())\n",
    "        sentencia.append(s)\n",
    "\n",
    "  df = pd.DataFrame({\n",
    "    'Sentencia': sentencia,\n",
    "    'Palabra': palabra,\n",
    "    'Tag': tag\n",
    "  })\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la usamos para abrir el conjunto de datos de prueba y de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = carga_dataframe(\"esp.train\")\n",
    "test_raw = carga_dataframe(\"esp.testb\")\n",
    "\n",
    "print(f\"Palabras únicas en el texto: {train_raw.Palabra.nunique()}\")\n",
    "print(f\"Sentencias: {train_raw.Sentencia.max()}\")\n",
    "print(f\"Etiquetas diferentes: {train_raw.Tag.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las etiquetas que tenemos son las siguientes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_raw.Tag.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesando la información \n",
    "\n",
    "Para el preprocesamiento, vamos a encontrar los índices de cada uno de las palabras de nuestro vocabulario, así como los índices de cada una de las etiquetas. A las palabras se le agregan dos más, la palabra para un token desconocido, y la que indica que se acabó la frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras = list(set(train_raw.Palabra.values))\n",
    "palabras.append(\"ENDPAD\")\n",
    "palabras.append(\"UNK\")\n",
    "word2idx = {w: i + 1 for i, w in enumerate(palabras)}\n",
    "num_palabras = len(palabras)\n",
    "\n",
    "tags = list(set(train_raw.Tag.values))\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "num_tags = len(tags)\n",
    "\n",
    "print(num_palabras, num_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver que al rededor de la cuarta parte de las palabras usadas en el conjunto de prueba, no se tienen en el conjunto de entrenamiento, por lo que este método va a ser muy limitado y es necesario extenderlo, usando vectores de palabras preentrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabrasOOV = set(test_raw.Palabra.values) - set(train_raw.Palabra.values)\n",
    "len(palabrasOOV)/ len(set(test_raw.Palabra.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con los diccionarios de palabras a índices y de tags a índices, podemos entonces convertir nuestro dataframe en una lista, donde cada entrada sea una frase completa de pares ordenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_fun(df):\n",
    "  return [\n",
    "      (p, t) \n",
    "      for (p, t) in zip(\n",
    "                        df.Palabra.values.tolist(),\n",
    "                        df.Tag.values.tolist()\n",
    "                       )\n",
    "  ]\n",
    "\n",
    "train_agg = train_raw.groupby('Sentencia').apply(agg_fun)\n",
    "test_agg = test_raw.groupby('Sentencia').apply(agg_fun)\n",
    "\n",
    "\n",
    "train_ls = [s for s in train_agg]\n",
    "test_ls = [s for s in test_agg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos lo que sale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿De que tamaño son cada una de las sentencias? Veamos en un histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(s) for s in train_ls], bins=50)\n",
    "plt.title('Distribución de tamaño de las sentencias')\n",
    "plt.xlabel('Tamaño en tokens')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos en el histograma, hay una muy pocas sentencias muy largas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(s) for s in train_ls if len(s) > 150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así que vamos a recortarlas todas a un máximo de 150 para ver un histograma más claro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ls = [s if len(s) < 150 else s[:150] for s in train_ls]\n",
    "\n",
    "plt.hist([len(s) for s in train_ls], bins=50)\n",
    "plt.title('Distribución de tamaño de las sentencias')\n",
    "plt.xlabel('Tamaño en tokens')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que nos lleva a pensar que con sentencias de unos 80 tokens como máximo cubrimos casi todas las sentencias.\n",
    "\n",
    "\n",
    "Ahora si, vamos a desarrollar unas secuencias de tokens de entrada y de datos de salida para hacer nuestro entrenamiento. Por cada sentencia, vamos a generar una secuencia de tokens de entrada y otra de salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_len = 80\n",
    "\n",
    "# Convertimos las palabras a tokens\n",
    "X_idx = [[word2idx[w[0]] for w in s] for s in train_ls]\n",
    "# Generamos las secuencias de entrada\n",
    "X = keras.preprocessing.sequence.pad_sequences(\n",
    "      maxlen=max_len, \n",
    "      sequences=X_idx, \n",
    "      padding=\"post\", \n",
    "      value=num_palabras-1\n",
    ")\n",
    "\n",
    "# Convertimos los tags a tokens\n",
    "y_idx = [[tag2idx[w[1]] for w in s] for s in train_ls]\n",
    "# Generamos la secuencia de salidas\n",
    "y = keras.preprocessing.sequence.pad_sequences(\n",
    "      maxlen=max_len, \n",
    "      sequences=y_idx, \n",
    "      padding=\"post\", \n",
    "      value=tag2idx[\"O\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y por últomo separamos nuestras secuencias en datos de entrenamiento y validación con `sklearn` como de costumbre "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo y entrenamiento\n",
    "\n",
    "El modelo es muy sencillo y podemos discutirlo mucho y modificarlo para probar nuevas cosas.\n",
    "\n",
    "Veamos el modelo, el cual lo vamos a definir en forma funcional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(max_len,))\n",
    "\n",
    "x = layers.Embedding(\n",
    "    input_dim=num_palabras, \n",
    "    output_dim=64, \n",
    "    input_length=max_len\n",
    ")(inputs)\n",
    "\n",
    "x = layers.SpatialDropout1D(0.1)(x)\n",
    "\n",
    "x = layers.Bidirectional(\n",
    "      layers.LSTM(units=126, return_sequences=True, recurrent_dropout=0.1)\n",
    ")(x)\n",
    "\n",
    "outputs = layers.TimeDistributed(\n",
    "      layers.Dense(num_tags, activation=\"softmax\")\n",
    ")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y lo compilamos para que esté listo para el entrenamiento. Recuerda que si la salida del modelo es una *softmax* del tamaño de las salidas, pero la salida que tienes es simplemente el indice que debe ser (sin haber hecho *one hot encoding*) entonces tienes que seleccionar como función de pérdida `sparse_categorical_crossentropy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para darle bien seguimiento vamos a establecer dos *checkpoints*, uno para ir guardand o los resultados intermedios, y otro para la detención prematura. Ninguno de los dos se necesita en este caso particular, pero no importa, vamos usándolo así por completud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = keras.callbacks.ModelCheckpoint(\n",
    "    \"model_weights.h5\", \n",
    "    monitor='val_loss',\n",
    "    verbose=1, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=True, \n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    min_delta=0, \n",
    "    patience=1, \n",
    "    verbose=0, \n",
    "    mode='max', \n",
    "    baseline=None, \n",
    "    restore_best_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora si, a poner a aprender este modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    validation_data=(x_val,y_val),\n",
    "    batch_size=32, \n",
    "    epochs=3,\n",
    "    callbacks=[chkpt, early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificación del modelo\n",
    "\n",
    "Vamos primero graficando la pérdida y la métrica que decidimos mantener a partir de la información histórica del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], '-o')\n",
    "plt.plot(history.history['val_accuracy'], '-o')\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['entrenamiento', 'validación'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], '-o')\n",
    "plt.plot(history.history['val_loss'], '-o')\n",
    "plt.title('Pérdida durante el aprendizaje')\n",
    "plt.ylabel('pérdida')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['entrenamiento', 'validación'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a ver como se comporta una sentencia en particular (la que sea). Así que vamos a simular con alguna secuencia que venga en el conjunto de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, x_val.shape[0]) \n",
    "\n",
    "x_i = np.array([x_val[i]])\n",
    "y_true = y_val[i]\n",
    "\n",
    "prediction = model.predict(x_i)\n",
    "y_est = np.argmax(prediction, axis=-1)[0]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"Palabra\".ljust(15) + \"Real\".ljust(10) + \"Estimada\" )\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "for w, true, pred in zip(x_i[0], y_true, y_est):\n",
    "  print( palabras[w-1].ljust(15) + \n",
    "         tags[true].ljust(10) + \n",
    "         tags[pred]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer la verificación mas dificil de hacer, la que incluye a otro conjunto de prueba. Para esto tenemos que procesar el conjunto de prueba que ya tenemos bien identificado desde el inicio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xt_idx = [[word2idx.get(w[0], num_palabras - 1) for w in s] for s in test_ls]\n",
    "Xt = keras.preprocessing.sequence.pad_sequences(\n",
    "      maxlen=max_len, \n",
    "      sequences=Xt_idx, \n",
    "      padding=\"post\", \n",
    "      value=num_palabras-1\n",
    ")\n",
    "\n",
    "yt_idx = [[tag2idx[w[1]] for w in s] for s in test_ls]\n",
    "yt = keras.preprocessing.sequence.pad_sequences(\n",
    "      maxlen=max_len, \n",
    "      sequences=yt_idx, \n",
    "      padding=\"post\", \n",
    "      value=tag2idx[\"O\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y evaluamos el modelo con nuevos datos nunca vistos para el algorítmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(Xt,yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recuerda que estos datos tienen muchos valores que no conocia el método (los tokens tipo OOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(0, Xt.shape[0]) \n",
    "\n",
    "x_i = np.array([Xt[i]])\n",
    "y_true = yt[i]\n",
    "\n",
    "prediction = model.predict(x_i)\n",
    "y_est = np.argmax(prediction, axis=-1)[0]\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"Palabra\".ljust(15) + \"Real\".ljust(10) + \"Estimada\" )\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "\n",
    "for w, true, pred in zip(x_i[0], y_true, Y_est):\n",
    "  print( palabras[w-1].ljust(15) + \n",
    "         tags[true].ljust(10) + \n",
    "         tags[pred]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea\n",
    "\n",
    "Vamos resolviendo el problema de los OOV, cambiando el uso de un vector de *embeddings* que entrenamos a usar directamente un vector preentrenado y bien conocido, usando [*GLOVe*](https://nlp.stanford.edu/projects/glove/), [*Word2Vec*](https://www.tensorflow.org/text/tutorials/word2vec) o [*FastText*](https://fasttext.cc) entre otros.\n",
    "\n",
    "Varios *embeddings* se pueden encontrar [en este repositorio de GitHub](https://github.com/dccuchile/spanish-word-embeddings). Aunque para *FastText* te recomiendo usar la [librería propia](https://fasttext.cc/docs/en/unsupervised-tutorial.html), así como el [modelo preentrenado oficial en español](https://fasttext.cc/docs/en/crawl-vectors.html). *Glove* y *Word2vec* se pueden usar sin problema como dccionarios, o de forma eficiente con la librería [Gensim](https://radimrehurek.com/gensim/index.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
