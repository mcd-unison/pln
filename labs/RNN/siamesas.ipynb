{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<p><img src=\"https://mcd.unison.mx/wp-content/themes/awaken/img/logo_mcd.png\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "<h1>Curso Procesamiento de Lenguaje Natural</h1>\n",
    "\n",
    "<h3>Redes siamesas con modelos preentrenados</h3>\n",
    "\n",
    "\n",
    "<p> Julio Waissman Vilanova </p>\n",
    "<p>\n",
    "<img src=\"https://identidadbuho.unison.mx/wp-content/uploads/2019/06/letragrama-cmyk-72.jpg\" width=\"150\">\n",
    "</p>\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mcd-unison/pln/blob/main/labs/RNN/siamesas.ipynb\"><img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\"  width=\"30\" /> Ejecuta en Colab</a>\n",
    "\n",
    "Tomado parcialmente y adaptado de [esta entrada de medium](https://medium.com/swlh/few-shot-learning-in-nlp-use-siamese-networks-189de22459d0) \n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "print('Tensorflow version:', tf.__version__)\n",
    "print('GPU detected:', tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos utilizados para el aprendizaje\n",
    "\n",
    "Vamos a utilizar un conjunto de textos que pertenecesn a 5 clases deferentes provenientes de blogs para ilustrar el uso de redes siamesas y las medidas de similaridad.\n",
    "\n",
    "Vamos a asumir que tenemos un conjunto pequeño de datos (100) pertenecientes a 5 clases diferentes, y con estos datos aprender una medida de similaridad que nos permita clasificar correctamente un grupo mucho más amplio de textos, utilizando el concepto de redes siamesas y de entrenamiento por tripletas.\n",
    "\n",
    "Veamos los datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = \"https://raw.githubusercontent.com/koushikkonwar/Few-Shot-/master/Dataset/final_fewshot_train.csv\"\n",
    "test_url = \"https://raw.githubusercontent.com/koushikkonwar/Few-Shot-/master/Dataset/final_fewshot_test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_url) \n",
    "test_df = pd.read_csv(test_url)\n",
    "\n",
    "print(\"Info del conjunto de entrenamiento:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\nInfo del conjunto de prueba:\")\n",
    "print(test_df.info())\n",
    "\n",
    "\n",
    "print(\"\\nClases del conjunto de entrenamiento:\")\n",
    "print(train_df.value_counts(\"class\"))\n",
    "\n",
    "for i in range(5):\n",
    "  print(f\"\\nTextos de la clase {i + 1}:\\n\")\n",
    "  print(train_df[train_df[\"class\"] == i + 1].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de la información\n",
    "\n",
    "Vamos a convertir una secuencia de tokens en inglés en una secuencia de *embeddings* utilizando un modelo preentrenado conocido como [Universal Sentence Encoder]() y el cual lo podemos consultar en su [enlace en TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4). Este es un modelo ligero y previo a *ELMo*, *BERT* y demás fauna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/4'\n",
    "embed = hub.load(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ejemplo de uso del USE\")\n",
    "print(f\"Sentencia: {train_df['text'].values[5]}\")\n",
    "print(f\"Embedding: {embed(train_df['text'].values[5:6])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de vector de similaridad\n",
    "\n",
    "Vamos a utilizar un modelo muy simple, es simplemente para agregar capas entrenables no recurrentes, pero si se decide se podrían modificar inclusive las capas del modelo original haciendo *fine-tunning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = keras.Input(shape=(512,))\n",
    "\n",
    "x = layers.Dense(256, activation='relu', name='d_1')(input_text)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.BatchNormalization(name='norm_1')(x)\n",
    "x = layers.Dense(\n",
    "        64, activation='relu', name='d_2',\n",
    "        kernel_regularizer=keras.regularizers.l2(0.001)\n",
    "    )(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "\n",
    "y = layers.Dense(128, name='d_3')(x)\n",
    "\n",
    "\n",
    "norm_layer = layers.Lambda(\n",
    "        lambda  x: keras.backend.l2_normalize(x, axis=1), \n",
    "        name='norm_2'\n",
    "    )(y)\n",
    "\n",
    "model=keras.Model(inputs=[input_text], outputs=norm_layer)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo para el aprendizaje y función de pérdida\n",
    "\n",
    "El modelo de aprendizaje con tripletas utiliza el modelo original 3 veces y cuando modifica sus pesos lo hace en forma global. Igualmente, la función de pérdida utilizada es particular, como lo vimos en clase, por lo que vamos a utilizar un modelo específico para aprender, pero derivado del modelo original. Cuando aprendamos en el modelo de aprendizaje, estamos ajustando automáticamente los pesos en nuestro modelo original.\n",
    "\n",
    "Iniciemos por establecer el modelo de aprendizaje:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos una capa de tripletas para la función de pérdida\n",
    "class TripletLossLayer(layers.Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist =  keras.losses.cosine_similarity(a, p, axis=-1)\n",
    "        n_dist = keras.losses.cosine_similarity(a, n, axis=-1)\n",
    "        return keras.backend.sum(\n",
    "            keras.backend.maximum(p_dist - n_dist + self.alpha, 0), \n",
    "            axis=0\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Entradas para anchor, pos y neg\n",
    "in_a = keras.Input(shape=(512,), name=\"in_a\")\n",
    "in_p = keras.Input(shape=(512,), name=\"in_p\")\n",
    "in_n = keras.Input(shape=(512,), name=\"in_n\")\n",
    "\n",
    "# Se aplica el modelo a las 3 entradas (red siamesa)\n",
    "emb_a = model(in_a)\n",
    "emb_p = model(in_p)\n",
    "emb_n = model(in_n)\n",
    "\n",
    "# Se agrega la capa de pérdida por tripletas\n",
    "triplet_loss_layer = TripletLossLayer(\n",
    "    alpha=0.4, \n",
    "    name='triplet_loss_layer'\n",
    ")([emb_a, emb_p, emb_n])\n",
    "\n",
    "\n",
    "model_train = keras.Model([in_a, in_p, in_n], triplet_loss_layer)\n",
    "     \n",
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haciendo un generador de secuencias para el aprendizaje\n",
    "\n",
    "Para acelerar el aprendizaje, vamos a realizar un generador de secuencias, aunque vamos a guardar las secuencias en memoria al no ser demasiadas, para que podamos trabajar con GPUs, paralelizando el aprendizaje. Así que le vamos a cargar toda la mano en el constructor de la clase, que al tener dos ciclos anidados muy feos, va a tardar un buen rato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletsBatches(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, batch_num, df, embed):\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_num = batch_num\n",
    "\n",
    "        labels = df['class'].unique()\n",
    "\n",
    "        y = df['class'].values\n",
    "        idxs_clase = {\n",
    "              label: np.flatnonzero(y == label) \n",
    "              for label in labels\n",
    "        }\n",
    "\n",
    "        self.a = []\n",
    "        self.p = []\n",
    "        self.n = []\n",
    "        \n",
    "        for _ in range(batch_num):\n",
    "          id_a, id_p, id_n = [], [], []\n",
    "          for _ in range(batch_size):\n",
    "            e_pos, e_neg = np.random.choice(labels, 2, replace=False)\n",
    "            ai, pi = np.random.choice(idxs_clase[e_pos], 2, replace=False)\n",
    "            ni = np.random.choice(idxs_clase[e_neg])\n",
    "            id_a.append(ai)\n",
    "            id_p.append(pi)\n",
    "            id_n.append(ni)\n",
    "        \n",
    "          self.a.append(embed(df.text.values[id_a]))\n",
    "          self.p.append(embed(df.text.values[id_p]))\n",
    "          self.n.append(embed(df.text.values[id_n]))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      return [self.a[idx], self.p[idx], self.n[idx]], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batch_num = 10\n",
    "\n",
    "secuencias_aprendizaje = TripletsBatches(batch_size, batch_num, train_df, embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Esto ya es de rutina..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(\n",
    "    loss=None, \n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "history = model_train.fit(\n",
    "    secuencias_aprendizaje, \n",
    "    epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.plot(history.history['loss'], '-')\n",
    "plt.title('Pérdida durante el aprendizaje')\n",
    "plt.ylabel('pérdida')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando el modelo para encontrar similaridades en un conjunto grande de datos\n",
    "\n",
    "Esta parte puede tardar un poco en ejecución, así que hay que tener calma. \n",
    "\n",
    "Vamos a hacer lo siguiente:\n",
    "\n",
    "1. Los textos de entrenamiento y prueba los vamos a convertir a embeddings, y a partir de ahí, a un vector de 128 dimensiones con nuestro modelo entrenado con la red siamesa.\n",
    "2. Vamos a usar los datos de entrenamiento como base en un clasificador por vecinos próximos\n",
    "3. Vamos a probar si encuentra una clasificación decente en un conjunto mucho más grande de datos de prueba (comparando las distancias coseno con los datos originales)\n",
    "\n",
    "Bueno, por partes, como decía Jack *el destripador*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los vectores y clases correspondientes \n",
    "# de los 100 datitos de entrenamiento\n",
    "\n",
    "X_train = model.predict(embed(train_df['text'].values)['outputs'])\n",
    "y_train = train_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obteniendo los vectores y clases correspondientes \n",
    "# del conjunto de prueba\n",
    "\n",
    "X_test = model.predict(embed(test_df['text'].values)['outputs'])\n",
    "y_test = test_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora vamos a ver si el vecino más próximo\n",
    "# le corresponde la misma clase\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='cosine')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Será? Vamos a tratar de ver que pasa y quienes son los vecinos de un texto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.random.randint(0, X_test.shape[0])\n",
    "\n",
    "x = X_test[ind:ind+1,:]\n",
    "\n",
    "print(f\"Texto: \\n{test_df.text.values[ind]}\\n\")\n",
    "\n",
    "dist, indices = knn.kneighbors(X=x, n_neighbors=5, return_distance=True)\n",
    "\n",
    "vecinos = pd.DataFrame({\n",
    "    'Texto': test_df.text.values[indices][0].tolist(),\n",
    "    'Distancia': dist[0].tolist()\n",
    "})\n",
    "\n",
    "vecinos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
