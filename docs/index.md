---
title: Curso Procesamiento de Lenguaje Natural 
subtitle: Maestría en Ciencia de Datos, Universidad de Sonora
layout: page
hero_image: https://github.com/mcd-unison/pln/raw/main/docs/img/banner-poema.jpg
hero_darken: true
show_sidebar: false
---


**Profesores**: 

- Olivia Gutú (olivia.gutu@unison.mx)
- Julio Waissman (julio.waissman@unison.mx)

**Horarios**:

- Lunes y jueves de 18:00 a 20:00 hrs
- Sábados 3 horas autónomas


**Lugar**: Laboratorio 202, edificio 3K-4

## Temario

### Etapa 1: Conceptos y teoría clásica de PLN

**Olivia Gutú**

| Fecha | Lunes 18:00 a 20:00 hrs      | Fecha | Jueves de 18:00 a 20:00 hrs     |
| ----- | ---------------------------- | ----- | ------------------------------- |
| 7/8   | Introducción al curso        | 10/8  | Taller expresiones regulares    |
| 14/8  | Normalización y tokenización | 17/8 | Taller de preprocesamiento (EDA) |
| 21/8  | Distancia de edición mínima  | 24/8 | Taller de autocorrector de Norvig |
| 28/8  | Modelo de $n$-gramas         | 31/8 | Taller de $n$-gramas             |
| 4/9   | NB, LR, bolsa de palabras    | 7/9  | Taller de modelos generativos vs discriminativos |
| 11/9  | Algoritmo de Viterbi (pos--tagging) | 14/9 | Vecinos próximos aproximados |
| 18/9  | Modelo CBOW                  | 21/9 | Modelos Word2Vec                  |

### Etapa 2: Modelos basados en aprendizaje profundo para PLN

**Julio Waissman**

| Fecha | Lunes 18:00 a 20:00 hrs               | Fecha | Jueves de 18:00 a 20:00 hrs     |
| ----- | ------------------------------------- | ----- | ------------------------------- |
| 2/9   | Modelos secuenciales RNN              | 5/9   | Unidades LSTM                   |
| 9/9   | Redes siamesas y *One shoot learning* | 12/9  | Modelos *seq-to-seq* y el mecanismo de atención |
| 16/9  | Atención es todo lo que necesitas     | 19/9  | Transferencia de aprendizaje    |
| 23/9  | Aprendizaje autosupervisado (BERT)    | 26/8  | Modelos preentrenados y ajuste fino |
| 30/9  | Explorando HF y sus aplicaciones      | 2/11  | Día de muertos (descanso)|
| 6/11  | Ventanas grandes de contexto          | 9/11  | Reformadores (transformadores en esteroides)|
| 13/11 | Uso de Modelos grandes de lenguaje (LLM)   | 15/11  | Ajuste fino tipo PEFT para LLM|

Las semanas que restan vamos a tratar modelos particulares para los proyectos de los estudiantes (o, si es necesario, le dedicaremos más tiempo del programado a algún tema que no hubiera sido agotado)



## Libro de consulta

[Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/)
Dan Jurafsky and James H. Martin
Stanford University



